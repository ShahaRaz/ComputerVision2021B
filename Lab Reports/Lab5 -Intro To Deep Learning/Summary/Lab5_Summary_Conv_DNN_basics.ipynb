{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab5_Summary_Conv_DNN_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1pzOBXgDPM"
      },
      "source": [
        "# **Lab 5 - Introduction to Deep learning**\n",
        "# **Computer Vision (10224)**\n",
        "<img src='https://upload.wikimedia.org/wikipedia/he/thumb/9/94/%D7%A1%D7%9E%D7%9C_%D7%94%D7%9E%D7%9B%D7%9C%D7%9C%D7%94.jpg/560px-%D7%A1%D7%9E%D7%9C_%D7%94%D7%9E%D7%9B%D7%9C%D7%9C%D7%94.jpg'></img>\t\n",
        "## **Goals**\n",
        "* Motivation to use nn frameworks such as pytorch, tensorflow etc.  \n",
        "\n",
        "\n",
        "## **Lab Session**\n",
        "1. what is pytorch\n",
        "2. Basic Deep learning\n",
        "  * Fully Connected layers \n",
        "  * Activation functions   \n",
        "3. MNIST\n",
        "  * Download the  MNIST Dataset\n",
        "  * Initlaize a torchvision pre-defined MNIST dataset class\n",
        "4. Single Fully connected model \n",
        "  * Build a model using single fully connected layer\n",
        "  * Demo evaluation of the randomly initialized weights \n",
        "    * A little bit about torch.nn.fanctional \n",
        "    * A little bit about softmax activation function\n",
        "5. Three FC-Layer model \n",
        "  * concatinate 3 FC layer with the following dimensions: \n",
        "    * FC-1: input: size of the image in pixels, output: 128\n",
        "    * FC-2: input: 128, output: 64\n",
        "    * FC-1: input: 64, output: number of classes to predict from ( in this case 10) \n",
        "6. Convolution layer\n",
        "7. Pooling layer \n",
        "8. MNIST Model with convolution layers \n",
        "9. Class Exercise build LeNet Architectue:\n",
        "\n",
        "<img src = \"https://d2l.ai/_images/lenet.svg\"></img> \n",
        "\n",
        "\n",
        "\n",
        "## **Final Report**\n",
        "1. Complete the unfinished lab session tasks.\n",
        "2. Write a short TL;DR (too long didn’t read) summary and describe your work and what you understood in the lab.\n",
        "\n",
        "## **Guidelines**\n",
        "1. Code written in the assignments MUST follow the PEP-8 guidelines, we will deduct points of code not following this guideline, points will be accumulated.\n",
        "2. You may use either Google Colab or Local Jupyter notebook session.\n",
        "3. Use Markdown cells in-between your code cells to explain what you’ve done.\n",
        "4. Assignments must be submitted as .html with all the required plots.\n",
        "Some exercises will require online reading, you may use Google and stackoverflow for inspiration, although if you use it make sure to attach the reference link. \n",
        "5. Add as much comments you need to explain yourself, you wouldn’t want to assume we fully understand your\n",
        "intuition.\n",
        "6. Code should be well ordered, formatted and readable.\n",
        "\n",
        "## **Further Reading**:\n",
        "[DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj2abcS42Y5Q"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0agl-SXLCgYj"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "# from google.colab import drive\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hcmVZoVCkuc"
      },
      "source": [
        "## What is PyTorch\n",
        "**PyTorch is very similar to Numpy and is mostly used as a deep learning research platform that provides maximum flexibility and speed e.g.** \n",
        "\n",
        "```\n",
        "# np.zeros == torch.zeros\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkvLMCs7DDGa"
      },
      "source": [
        "**Let see some of the similarities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLW5N1YmC_yR"
      },
      "source": [
        "shape = (3, 3, 3)\n",
        "\n",
        "# initialize a zero array in numpy\n",
        "zeros_np = np.zeros(shape)\n",
        "\n",
        "# initialize a zero array in pytorch\n",
        "zeros_torch = torch.zeros(shape)\n",
        "\n",
        "# initialize a zero array in torch from numpy\n",
        "zeros_torch_from_np = torch.from_numpy(zeros_np)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBkDvz4iDuqF"
      },
      "source": [
        "**Random numbers generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxwXqWG5DhKq"
      },
      "source": [
        "# set random seed\n",
        "torch.manual_seed(7)\n",
        "\n",
        "# initialize a random numbers array in numpy\n",
        "randn_np = np.random.randn(*(shape))\n",
        "\n",
        "# initialize a random numbers array in pytorch\n",
        "randn_torch = torch.randn(shape)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVjOfkpjEyW5"
      },
      "source": [
        "**Auto gradient**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pfAMIhzEQEJ",
        "outputId": "49fd7945-cb89-48f2-95a4-8a24fc5c42dd"
      },
      "source": [
        "x = np.arange(0, 10, 2, dtype=np.float32)\n",
        "print('x:', x)\n",
        "x_torch = torch.from_numpy(x)\n",
        "print ('x as troch:', x_torch)\n",
        "x_torch = x_torch.cuda()\n",
        "print ('x as troch to cuda', x_torch)\n",
        "x_torch.requires_grad = True\n",
        "y = 2 * (x_torch**2)\n",
        "print('y:', y)\n",
        "z = y.sum()\n",
        "print('z:', z)\n",
        "\n",
        "print('------------')\n",
        "# lets get the gradients.\n",
        "z.backward()\n",
        "print(x_torch.grad)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: [0. 2. 4. 6. 8.]\n",
            "x as troch: tensor([0., 2., 4., 6., 8.])\n",
            "x as troch to cuda tensor([0., 2., 4., 6., 8.], device='cuda:0')\n",
            "y: tensor([  0.,   8.,  32.,  72., 128.], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "z: tensor(240., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "------------\n",
            "tensor([ 0.,  8., 16., 24., 32.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5U9oGagb09H"
      },
      "source": [
        "**Torch to numpy**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKS0GhAHcanJ"
      },
      "source": [
        "# from torch to numpy\n",
        "x_np = x_torch.detach().cpu().numpy()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv6jFJLIcqGf"
      },
      "source": [
        "**Using the GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op2-L1IScu_K",
        "outputId": "6f7ec42a-9336-4f61-be23-fb5a16ba8910"
      },
      "source": [
        "# First we check if we have a GPU which can use\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "\n",
        "# Now we move the tensor from the cpu to GPU\n",
        "x_torch.to(device)\n",
        "\n",
        "#Or we can use the otherway which is:\n",
        "\n",
        "#x_torch.cuda() # this will throw and error if cuda device is not available"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6., 8.], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sle8szZMIl-5"
      },
      "source": [
        "## Basic Deep learning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNy-2iMrIqOe"
      },
      "source": [
        "### Fully-Connnected layer\n",
        "$x_{in}*weights + biases$\n",
        "\n",
        "**Shapes:**\n",
        "$[Batch_{in}, featuresSize] * [featuresSize, OutSize] + [N] = [Batch_{in}, OutSize]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GVEXgliIgH2"
      },
      "source": [
        "# Old school low level fully connected layer\n",
        "batch_size = 1 \n",
        "features_size = 28\n",
        "out_size = 2\n",
        "x_input = torch.randn((batch_size, features_size)) \n",
        "weights = torch.randn((features_size, out_size)) # random init weights, never set weights to zero.\n",
        "biases = torch.zeros(out_size) # set biases to zero"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS9MQYXEJMmg"
      },
      "source": [
        "# just like the above formula\n",
        "fc1 = torch.matmul(x_input, weights) + biases\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTXNTeRiM-lk",
        "outputId": "0498a2d0-7f0a-4894-d6de-e9518de53b59"
      },
      "source": [
        "# How we do it in PyTorch\n",
        "our_first_model = nn.Sequential(\n",
        "    nn.Linear(in_features=features_size, out_features=out_size, bias=True),\n",
        ")\n",
        "# Run it !\n",
        "print(our_first_model(x_input))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4492, 1.3131]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkHpDVj9fkgv",
        "outputId": "f436a2db-2ff5-4ea7-ce32-9178371b7617"
      },
      "source": [
        "#2\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, features_size_in, out_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=features_size_in,\n",
        "                             out_features=out_size, bias=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc1(x)\n",
        "  \n",
        "\n",
        "# Run it!\n",
        "net = Model(features_size, out_size)\n",
        "print(net(x_input))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.7703, -0.8064]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5diCRaQ6NkE8"
      },
      "source": [
        "### Activation functions\n",
        "**What are they good for? Breaking linearity !**\n",
        "\n",
        "A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A Neural Network without Activation function would simply be a Linear regression Model, which has limited power and does not performs good most of the times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdNu_rXdOaRQ"
      },
      "source": [
        "Our first activation! the common **ReLU** stands for rectified linear unit\n",
        "it basicly does $max{(0, features)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5_ZE0wlNi-l"
      },
      "source": [
        "# the Torch way!\n",
        "relu1 = F.relu(fc1)\n",
        "# the numpy way!\n",
        "relu2 = np.maximum(0, fc1.numpy())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jXBuClRQ-Wg"
      },
      "source": [
        "## MNIST Dataset \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYSSAJFk54rO"
      },
      "source": [
        "### let's download the data \n",
        "!wget is used to download the data \n",
        "\n",
        "!tar is used to decompress the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ernf7lOu2deX",
        "outputId": "b796a359-043c-4d88-d0ab-70956519e923"
      },
      "source": [
        "#Download\n",
        "!wget -O MNIST.tar.gz https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.new.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-04 07:22:57--  https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.new.tar.gz\n",
            "Resolving activeeon-public.s3.eu-west-2.amazonaws.com (activeeon-public.s3.eu-west-2.amazonaws.com)... 52.95.150.98\n",
            "Connecting to activeeon-public.s3.eu-west-2.amazonaws.com (activeeon-public.s3.eu-west-2.amazonaws.com)|52.95.150.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34812527 (33M) [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz        100%[===================>]  33.20M  15.3MB/s    in 2.2s    \n",
            "\n",
            "2021-04-04 07:22:59 (15.3 MB/s) - ‘MNIST.tar.gz’ saved [34812527/34812527]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/processed/\n",
            "MNIST/processed/test.pt\n",
            "MNIST/processed/training.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3yGSnXy6L9o"
      },
      "source": [
        "### Load the data to the predefined torchvision MNIST Dataset class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmBQl6_VRGpU"
      },
      "source": [
        "trainset = torchvision.datasets.MNIST(root='', train=False,\n",
        "                                        download=False);\n",
        "                                        \n",
        "num_classes = len(trainset.classes)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P1PTf1A6XgC"
      },
      "source": [
        "#### Visualize one image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "tyeHo_qiROSG",
        "outputId": "2cdda56b-a6a1-4af4-a273-7d29685a0646"
      },
      "source": [
        "\n",
        "idx = np.random.randint(trainset.data.shape[0])\n",
        "plt.imshow(trainset.data[idx].numpy(), cmap=\"gray\")\n",
        "plt.title(f'The number {trainset.train_labels[idx]}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:54: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'The number 8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHgYL3bQORg"
      },
      "source": [
        "## Building a Model using a single FC - Fully-Connected layer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RHSz2-2QNJM"
      },
      "source": [
        "class mnistModel(nn.Module):\n",
        "    def __init__(self, features_size_in, out_size):\n",
        "        super(mnistModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=features_size_in, out_features=out_size)\n",
        "  \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        y = self.fc1(x)\n",
        "        return y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hsqA9186v53"
      },
      "source": [
        "get a random image from the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XHsmmAAiZMf"
      },
      "source": [
        "random_image = trainset.data[idx] / 255."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPEBf12pibFV"
      },
      "source": [
        "h, w = random_image.size()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjw9HOKFwCdu"
      },
      "source": [
        " ### Initilize  our FC MNIST model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKSiqVWRiMww"
      },
      "source": [
        "mnist_model = mnistModel(h * w, num_classes)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl_llRZuwJt8"
      },
      "source": [
        "#### Demo evaluation of the random initalized mnist fully connectem model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs4AJPBf7wjm"
      },
      "source": [
        "explantion of some of the method which has been used: \n",
        "\n",
        "<code>numpy()</code> method is used to convert the tensor to numpy array please notice that it has to be on the cpu otherwise an error will be raised \n",
        "\n",
        "<code>cpu()</code> method is used to copy the tensor to the cpu\n",
        "\n",
        "<code>torch.torch.nn.functional</code> allows us to use a nn function with out the need to pre- define it, it is common to use this module for small modification of a certion model output. \n",
        "\n",
        "<code>softmax()</code> applies the **softmax activation** function: \n",
        "softmax forces the sum of the prediction to be one and each prediction value to be between zero and one. Thus softmax repressent the probabilty of each class to be the corrent class in the input image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12nz1SdsiwyP"
      },
      "source": [
        "output = mnist_model(random_image[np.newaxis, :])\n",
        "softmax_output = F.softmax(output, dim=-1)\n",
        "output_np =  output.detach().cpu().numpy()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQB8vjtW0Jl-"
      },
      "source": [
        "let's review the result and different ways to print tensor \n",
        "\n",
        "<code>item()</code> method is used to extract the value of of a one element tensor.\n",
        "\n",
        "<code>detach()</code> method is used to remove the gradient backwards path calculation \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzkN-BEiVVqS",
        "outputId": "76beed53-f892-4797-cb1a-b66b5ff75a48"
      },
      "source": [
        "print('model output:', output.detach().cpu().numpy())\n",
        "print ('output with softmax',softmax_output.detach().cpu().numpy())\n",
        "print('sum (no softmax):', output.detach().sum().item(),\n",
        "      'sum (with softmax)',softmax_output.detach().sum().item())\n",
        "print ('predicted digit (no softmax)', output.detach().argmax())\n",
        "print ('predicted digit (with softmax)', softmax_output.detach().argmax())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model output: [[ 0.10815312  0.28318918 -0.3210346  -0.09450237  0.01807737  0.11282752\n",
            "  -0.14289083 -0.29256982 -0.10143402 -0.17986543]]\n",
            "output with softmax [[0.11646969 0.13874908 0.07582616 0.09510443 0.10643722 0.11701538\n",
            "  0.09061205 0.07801554 0.09444748 0.08732288]]\n",
            "sum (no softmax): -0.6100499033927917 sum (with softmax) 0.9999998807907104\n",
            "predicted digit (no softmax) tensor(1)\n",
            "predicted digit (with softmax) tensor(1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vRmcrw7j-2v"
      },
      "source": [
        "## Implement a 3 layer fully connected model:\n",
        "\n",
        "the layers demtion should be as follows:\n",
        "\n",
        "* for the first layer: output features of 128 \n",
        "* for the second layer: output features of 64 \n",
        "* for the third layer: the output features would be the number of classes to predict from, in this case 10.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk5a0hhdkI6Y"
      },
      "source": [
        "class mnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mnistModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=28 * 28, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features = 128, out_features=64)\n",
        "        self.fc3 = nn.Linear(in_features = 64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        y = F.sigmoid(x)\n",
        "        return y"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp7v5s7EkLhV"
      },
      "source": [
        "## Convolution layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82pdVkUekRB6"
      },
      "source": [
        "**Works more like correlation then convolution.**\n",
        "How does it  works? let us watch https://cs231n.github.io/convolutional-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwtKK6lIk4ew"
      },
      "source": [
        "**Conv2d  in PyTorch**\n",
        "Syntax : **nn.Conv2d(in_channels, out_channel, kernel_size, stride, padding)**\n",
        "**Note: in PyTorch we work with tensor that has shape like [batch_size, Channels, H, W] so we have to transpose the channels into the right order before processing the data.**\n",
        "\n",
        "Watch the example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfe5WyUzneSL",
        "outputId": "f5712a61-3632-4620-8e8f-1305973e85b2"
      },
      "source": [
        "input_image = random_image\n",
        "print(input_image.shape) # shape of only H, W  which means we have to add two more channels\n",
        "# batch_size if we only work with one image and the color channel.\n",
        "# Mostly when working with RGB/BGR images we wont have to add extra channels\n",
        "conv_ready_image = random_image[np.newaxis, np.newaxis, :] # we added one channel at the start and one after him\n",
        "print(conv_ready_image.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28, 28])\n",
            "torch.Size([1, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ8efApLldrE"
      },
      "source": [
        "conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), stride=1) # Create a conv2D layer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPpdVgbhmVTn",
        "outputId": "07e3dd65-c9c8-471f-bfb7-5aeacd5018b8"
      },
      "source": [
        "conv_res = conv1(conv_ready_image) # Run the conv2D layer on our input\n",
        "print(conv_res.shape) # see how the new shapes, H and W are reduced by 2 and the channels changed to 3"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8, 26, 26])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH8XujwUpPOw"
      },
      "source": [
        "## Pooling Layer\n",
        "Syntax: **nn.MaxPool2d(kernel_size)**\n",
        "This layer purpose is to reduce the spatial size, it is a sampling method.\n",
        "There's a window with size kernel_size that picks the maximum number in that window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P-1-8d5p5a3",
        "outputId": "0487696e-329c-441d-9637-350a865ce01e"
      },
      "source": [
        "pooling1 = nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
        "pooling_res = pooling1(conv_res) #use the layer!\n",
        "print(pooling_res.shape) # We reduced H and W by a factor of 2 from 26X26 to 13X13"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8, 13, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8r-2eHx9y-r"
      },
      "source": [
        "## MNIST Model with conv layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcrSfri9rj44"
      },
      "source": [
        "# Quick and small conv2d based model\n",
        "\n",
        "class mnistModelV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mnistModelV2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, (3, 3))\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(in_features=13*13*8, out_features=10)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        y = self.fc1(x)\n",
        "        return y"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSqcFCKCsXpi"
      },
      "source": [
        "mnist_v2 = mnistModelV2()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y95-EwUossLm",
        "outputId": "a890cda7-b25c-48d3-d658-90f6ae583fd2"
      },
      "source": [
        "improved_mnist_res = mnist_v2(conv_ready_image)\n",
        "print(improved_mnist_res.detach())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1340, -0.0275, -0.1635, -0.2697, -0.3509,  0.0845, -0.0536, -0.0826,\n",
            "          0.2743,  0.1047]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5ben2dFos5X"
      },
      "source": [
        "## **Class exercise build the LeNet architecture**\n",
        "\n",
        "<img src = https://d2l.ai/_images/lenet.svg></img> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX-iRrHaCY9z"
      },
      "source": [
        "**Architecture hyper parameters:**\n",
        "* conv1: in channels: 1, out channels: 6, kernel size: 5\n",
        "* pool1: kernel size: 2\n",
        "* conv2: in channels: 6, out channels: 16, kernel size: 5\n",
        "* pool2: kernel size: 2\n",
        "\n",
        "* FC1: in channels: 256, out channels: 120 \n",
        "* FC2: in channels: 120, out channels: 84\n",
        "* FC3: in channels: 84, out channels: 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk3S0setrmQI"
      },
      "source": [
        "# LeNet skeleton!\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # self.x = \"Input For Next Layer Will be here\"\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5, 5), padding = 2) # 1x28x28 -> 6x28x28 (padding = same)\n",
        "        self.pool1 = nn.MaxPool2d(2) # 6x28x28 -> 6x14x14\n",
        "        self.conv2 = nn.Conv2d(6, 16, (5, 5), padding =0 , stride=1) # 6x14x14 -> 16x10x10 (padding minimal..?)\n",
        "        self.pool2 = nn.MaxPool2d(2) # 16x10x10 -> 16x5x5 (2)\n",
        "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.view(x.shape[0],-1) # [28,28] -> [28x28]\n",
        "        ##___  conv1\n",
        "        x = self.conv1(x) # 6x28x28\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.pool1(x) # 6x14x14\n",
        "\n",
        "        x = self.conv2(x) # 16x10x10\n",
        "        x = F.relu(x)\n",
        "        print(f'25: x dimentions are: {x.shape}')\n",
        "        x = self.pool2(x)\n",
        "        print(f'26: x dimentions are: {x.shape}')\n",
        "        # from image to fully connected\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        y = F.sigmoid(x)\n",
        "        return y \n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X67PphP0e2Vj"
      },
      "source": [
        "le_net_shahar = LeNet()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ttspyxae1Mt",
        "outputId": "7f591991-624d-4aba-a887-7fa8387aa98e"
      },
      "source": [
        "le_net_object = le_net_shahar(conv_ready_image)\n",
        "print(le_net_object.detach())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25: x dimentions are: torch.Size([1, 16, 10, 10])\n",
            "26: x dimentions are: torch.Size([1, 16, 5, 5])\n",
            "tensor([[0.5006, 0.5363, 0.4756, 0.4821, 0.4946, 0.5161, 0.5211, 0.4858, 0.5190,\n",
            "         0.5094]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khIhqi93wI_1"
      },
      "source": [
        "#TL;DR\n",
        "We've learned how to build Neural network, adding fully connected and convolutional layers, connecting them into a single Neural network and do a forward pass calculations with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcLPLLl2fFQi"
      },
      "source": [
        "%%shell\n",
        "\n",
        "jupyter nbconvert --to html /content/CV2021Bipynb"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}