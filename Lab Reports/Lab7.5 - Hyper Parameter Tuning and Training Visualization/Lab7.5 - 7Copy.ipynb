{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab7.5 - 7Copy.ipynb","provenance":[{"file_id":"1aJOXicS0xi3fuA8l7Y20P_5hAFerHvdf","timestamp":1619369179054},{"file_id":"1dVaO4j2RMECa5bhwzgwIYfCbOjbLspfG","timestamp":1618211016782}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPpQ/HnX/AafqhXlEazJYA5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d120651125294f9a8c9fd3f82a622b82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9e51866f7f824baa962b66e1f9485a51","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_31a295d0dda640dda5772518b512e78a","IPY_MODEL_bb8bed57fcdd4984babd4325412e9391"]}},"9e51866f7f824baa962b66e1f9485a51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"31a295d0dda640dda5772518b512e78a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fad6c0318f114a12b603cbd23e6d9fc0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8af6b20453564d7292d38de7b5f67322"}},"bb8bed57fcdd4984babd4325412e9391":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_742b87eda12c4d8a8495da9ed124b0b0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:05&lt;00:00, 30831061.92it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d10b7d3896e4cb389193a51a005ab08"}},"fad6c0318f114a12b603cbd23e6d9fc0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8af6b20453564d7292d38de7b5f67322":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"742b87eda12c4d8a8495da9ed124b0b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d10b7d3896e4cb389193a51a005ab08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"WUHj53A5jM5O"},"source":["# Assigment Paper:\n","\n","![image](https://user-images.githubusercontent.com/55464049/114030959-8c7be680-9883-11eb-932b-44146b041417.png)\n","\n","\n","\n","\n","[Adam](https://arxiv.org/pdf/1412.6980v1.pdf)\n","\n","[AdamW](https://arxiv.org/abs/1711.05101v3)\n","\n","\n","[Focal Loss](https://arxiv.org/abs/1708.02002v2)\n","\n","[Dice](https://arxiv.org/pdf/1707.03237.pdf)\n","\n"," __UPDATE 14April2021__\n","\n"," ![image](https://user-images.githubusercontent.com/55464049/114354049-65206480-9b76-11eb-8a88-d5d1416ab0dd.png)\n"]},{"cell_type":"markdown","metadata":{"id":"2rGgUaNPFW1k"},"source":["# Lec Preperation"]},{"cell_type":"markdown","metadata":{"id":"VWqc9K2QkcFD"},"source":["##1. Optimizers Read & Summarize "]},{"cell_type":"markdown","metadata":{"id":"yFuQzxgeFbQS"},"source":["### 1.1 Adam\n","a method for efficient stochastic optimization that only requires first-order gradients and requires little memory.\n","\n","Consists of 2 main methods for reaching fast to the minimum.\n","\n","1. Momentum: keeping the general direction of the movement. helps avoiding local minimums, and acts like a ball goin down a hill.\n","![image](https://user-images.githubusercontent.com/55464049/114276061-b244ef00-9a2d-11eb-9c33-feaa59b257a7.png)\n","\n","\n","2. RMSProp: (a better version of AdaGrad which keeping track of a historical average of square of gradient values)\n","![image](https://user-images.githubusercontent.com/55464049/114275988-5e3a0a80-9a2d-11eb-854c-d0af3e60f02a.png)\n",", in RMSProp we also add friction (decay) \n","![image](https://user-images.githubusercontent.com/55464049/114275942-14e9bb00-9a2d-11eb-97b8-c200f235ba48.png)\n","which will not allow the values to explode.\n","The RMSProp arrests progress along the fast moving direction while simultaneously accelerating progress along slow moving directions, which helps it to bend directly towards the bottom.\n","\n","![image](https://user-images.githubusercontent.com/55464049/114276907-6d22bc00-9a31-11eb-854a-d903d36e5793.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Cfqvs_4kjOKA"},"source":["### 1.2 AdamW Optimizer\n","A simple modification from Adam to recover the original formulation of weight decay regularization by __decoupling__ the weight decay from the optimization steps taken. The weight decay is calculated _after_ the main calculation, and therefore does not interrupt with the them.\n","\n","[Link](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)\n","![image](https://user-images.githubusercontent.com/55464049/114277470-fe932d80-9a33-11eb-9158-9967d370654e.png)\n"]},{"cell_type":"markdown","metadata":{"id":"TzEDZJK2e6cb"},"source":["### 1.3 SGD\n","Basically, when we want to progress with our model:\n","* we can check one example, check the error, compute the gradients and change the network. but, one example can be biased in many ways, and contains an unknown amount of noise. __1 example__.\n","\n","* So in order to find patterns, and go in the right direction, most likely is that we'll want to check all of our examples, average the direction, compute the loss and backdrop according to all of it.\n","but, if we have a lot of data (millions-billions..) computing and averaging all the examples for each step will take VERY much time. __N examples__.\n","\n","* so, we try to find a middle way, here is where __SGD__ comes in! \n","We'll randomly subsample our dataset to groups who each have k samples in it.\n","So rather than computing the loss on the full data-set, in each step we'll compute the loss over a minibatch, and backprop accordingly. __n examples__.\n","\n","![image](https://user-images.githubusercontent.com/55464049/114276150-2384a200-9a2e-11eb-9c8d-fc12851f536c.png)\n"]},{"cell_type":"markdown","metadata":{"id":"-sE1buYMk8HP"},"source":["## 2. Optimizer Implemetations"]},{"cell_type":"markdown","metadata":{"id":"JLN62QIootAK"},"source":["### 2.1 AdamW Implementation"]},{"cell_type":"code","metadata":{"id":"VcfSUnjmjLG_"},"source":["# from: https://github.com/egg-west/AdamW-pytorch/blob/master/adamW.py\n","import math\n","import torch\n","from torch.optim.optimizer import Optimizer\n","\n","class AdamW(Optimizer):\n","    \"\"\"Implements Adam algorithm.\n","    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n","            algorithm from the paper `On the Convergence of Adam and Beyond`_\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _On the Convergence of Adam and Beyond:\n","        https://openreview.net/forum?id=ryQu7f-RZ\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0, amsgrad=False):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad)\n","        super(AdamW, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamW, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","                amsgrad = group['amsgrad']\n","\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","                    if amsgrad:\n","                        # Maintains max of all exp. moving avg. of sq. grad. values\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                # if group['weight_decay'] != 0:\n","                #     grad = grad.add(group['weight_decay'], p.data)\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    # Maintains the maximum of all 2nd moment running avg. till now\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    # Use the max. for normalizing running avg. of gradient\n","                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n","                else:\n","                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                # p.data.addcdiv_(-step_size, exp_avg, denom)\n","                p.data.add_(-step_size,  torch.mul(p.data, group['weight_decay']).addcdiv_(1, exp_avg, denom) )\n","\n","        return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYjq9UqvquoP"},"source":["### 2.2 Adam"]},{"cell_type":"code","metadata":{"id":"fUGDz16Yof04"},"source":["import torch\n","import math\n","from torch.optim import Adam, SGD\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBBWjGFhrkzQ"},"source":["### 2.3 Choose Optimizer:"]},{"cell_type":"code","metadata":{"id":"Rw354jIVrnOH"},"source":["LR = 0.01\n","BATCH_SIZE = 32\n","EPOCH = 12\n","WD = 0.1\n","\n","def set_optimizer(opt_name, params, learningRate):\n","    if opt_name == 'SGD':\n","        optimizer = optim.SGD(params, lr=learningRate)\n","    elif opt_name == 'Adam':\n","        optimizer = optim.Adam(params, lr=learningRate)\n","    elif opt_name == 'AdamW':\n","        optimizer = AdamW(params, lr=learningRate, betas=(0.9, 0.99), weight_decay = 0.1)\n","    else:\n","        print(f'no {opt_name} optimizer availble, choosing Adam by default.')\n","        optimizer = optim.Adam(params, lr=learningRate)\n","    \n","    if optimizer:\n","        return optimize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlyGZy2cq1LS"},"source":["## 3. Losses Read & Summarize "]},{"cell_type":"markdown","metadata":{"id":"0_EcZzjKrYzh"},"source":["### 3.1 Cross Entropy - CE\n","The Cross Entropy function is a way to measure the loss of classification problem. It must be activated after softmax function, which calculates the odds for specific class to be choosen. If the network is 100% sure of a class, it will give it score of 1, and in this case we will have no loss, which means that the NN will not be modified.\n","The Step size is accoding to the loss, if the loss is high, the CE derivetive will be steep, therefor the step size will be bigger.\n","\n","[Stat-Quest Video](https://www.youtube.com/watch?v=6ArSys5qHAU&ab_channel=StatQuestwithJoshStarmer)\n","\n","![image](https://user-images.githubusercontent.com/55464049/114303193-9563f680-9ad5-11eb-9a0b-f6cd93b6ddad.png)\n"]},{"cell_type":"markdown","metadata":{"id":"Tgp0QyMyrdfo"},"source":["### 3.2 Binary Cross Entropy with logits loss\n","\n","[What is Logits](https://stackoverflow.com/questions/34240703/what-is-logits-what-is-the-difference-between-softmax-and-softmax-cross-entropy)\n","\n","\n","* Logits:  the function operates on the unscaled output, that __the values are not probabilities__.\n","\n","* Binary Cross Entropy: Cross Entropy of only 2 categories.\n","\n","\n","[Pytorch Link:](https://pytorch.org/docs/master/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)\n","This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qrwzi6FctvAw"},"source":["### 3.3 Focal Loss\n","Focal Loss is a modulated CE function which reduces the loss for easy / common examples, whereas keep the loss for the hard / rare examples.\n","When __Gamma==0 <=> FocalLoss == CE__ , when gamma \n","\n","this approach is useful in many Image Classification Problems.\n","![image](https://user-images.githubusercontent.com/55464049/114303741-4f5c6200-9ad8-11eb-8edd-5ec9e6cfd095.png)\n","\n","Another way, apart from Focal Loss, to deal with class imbalance is to introduce weights. Give high weights to the rare class and small weights to the dominating or common class. These weights are referred to as α.\n","\n","![image](https://user-images.githubusercontent.com/55464049/114303812-aa8e5480-9ad8-11eb-9bf2-b56d8f29620b.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XJcJr2cAtxSS"},"source":["### 3.4 Dice\n","\n","Dice loss is based on the Sorensen-Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.\n","\n","Dice's formula:\n","![image](https://user-images.githubusercontent.com/55464049/114304068-e83fad00-9ad9-11eb-9bb9-3ab066dddd02.png)\n","\n","[Dice impelemtation:](https://www.jeremyjordan.me/semantic-segmentation/#:~:text=around%20the%20cells.%20(-,Source),denotes%20perfect%20and%20complete%20overlap.)"]},{"cell_type":"code","metadata":{"id":"chJqu5vv5TvD"},"source":["# from : https://www.jeremyjordan.me/semantic-segmentation/#:~:text=around%20the%20cells.%20(-,Source),denotes%20perfect%20and%20complete%20overlap.\n","def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n","    # skip the batch and class axis for calculating Dice score\n","    axes = tuple(range(1, len(y_pred.shape)-1)) \n","    numerator = 2. * np.sum(y_pred * y_true, axes)\n","    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n","    \n","    return 1 - np.mean((numerator + epsilon) / (denominator + epsilon)) # average over classes and batch\n","    # thanks @mfernezir for catching a bug in an earlier version of this implementation!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HtFZd5trGzd"},"source":["## 4. Losses Implementations:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R2LojdxJrUXg"},"source":["### 4.1  Focal Loss\n"]},{"cell_type":"code","metadata":{"id":"sVY-owMBrT9J"},"source":["def focal_loss(pt, alpha, gama):\n","    return (-1) * alpha * ((1 - pt)**gama) * np.log(pt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1svi_wdyhTav"},"source":["class FocalLoss(nn.Module):\n","    def __init__(self, alpha=0.2, gama=2, logist=False, reduce='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.cross_entropy_loss = nn.CrossEntropyLoss()\n","        self.reduce = reduce\n","\n","    def forward(self, inputs, targets):\n","        BCE_loss = self.cross_entropy_loss(inputs, targets)\n","        pt = torch.exp(-BCE_loss)\n","        F_losss = self.alpha * (1-pt) **self.gama * BCE_loss\n","        if self.reduce:\n","            return torch.mean(F_loss)\n","        return F_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKlJWRvAfGwy"},"source":["### Snippets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"mbyFeglcfJU6","executionInfo":{"status":"ok","timestamp":1619374552191,"user_tz":-180,"elapsed":9835,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"fe48f931-a368-4d2b-9d5f-94c2cdcf6e77"},"source":["import torch\n","import math\n","from torch.optim import Adam, SGD\n","from torch.optim.optimizer import Optimizer, required\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(D_in, H),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(H, D_out),\n",")\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","\n","learning_rate = 1e-4\n","\n","display(model.parameters())\n","for parm in model.parameters():\n","    print(f' {parm.shape} \\t')\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","for t in range(500):\n","\n","    y_pred = model(x)\n","\n","    loss = loss_fn(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<generator object Module.parameters at 0x7f7a0f461d50>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":[" torch.Size([100, 1000]) \t\n"," torch.Size([100]) \t\n"," torch.Size([10, 100]) \t\n"," torch.Size([10]) \t\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"],"name":"stderr"},{"output_type":"stream","text":["99 46.7319221496582\n","199 0.8154428601264954\n","299 0.026522746309638023\n","399 0.0012860035058110952\n","499 3.049271799682174e-05\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OYIpNCfR0vLd"},"source":["# IN Class Practice:"]},{"cell_type":"code","metadata":{"id":"7AaIbUUWfLAR"},"source":["import torch\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import tensorflow.keras.datasets.mnist as mnist\n","from torch.utils.data import Dataset , DataLoader\n","from sklearn.metrics import confusion_matrix\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import plot_confusion_matrix\n","from torch.autograd import Variable\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUVSzdu202ra","executionInfo":{"status":"ok","timestamp":1619374558005,"user_tz":-180,"elapsed":15622,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"16dec183-d6a8-4ad6-b645-615d10be70c1"},"source":["!pip install torchmetrics\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchmetrics\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/99/dc59248df9a50349d537ffb3403c1bdc1fa69077109d46feaa0843488001/torchmetrics-0.3.1-py3-none-any.whl (271kB)\n","\r\u001b[K     |█▏                              | 10kB 24.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 25.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 21.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 92kB 17.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 112kB 17.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 122kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 133kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 17.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 153kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 163kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 174kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 184kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 194kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 204kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 215kB 17.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 225kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 235kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 245kB 17.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 256kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 266kB 17.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 17.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.8.1+cu101)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (20.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (2.4.7)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.3.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bpYBknUc2eA8"},"source":["### How to step the optimizer"]},{"cell_type":"code","metadata":{"id":"agMfTrRz02jW"},"source":["import torchmetrics\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvHfkiAP05hh"},"source":["model = nn.Linear(1,1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjWPM9Mo1Ef3"},"source":["sgd = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9, nesterov=True)\n","# momentum : Parameter that accelerate SGD in relevant direction and dampens oscillations\n","sgd.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFQb76C21blI"},"source":["adam = optim.Adam(model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)\n","adam.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIVP_aNt19aI"},"source":["# adamw = optim.AdamW(model.parameters(),lr = 1e-3)\n","# adamw.step(closure=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Zvvjg3f2hGL"},"source":["### import data"]},{"cell_type":"code","metadata":{"id":"uvpRTFAk2Of4","colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["d120651125294f9a8c9fd3f82a622b82","9e51866f7f824baa962b66e1f9485a51","31a295d0dda640dda5772518b512e78a","bb8bed57fcdd4984babd4325412e9391","fad6c0318f114a12b603cbd23e6d9fc0","8af6b20453564d7292d38de7b5f67322","742b87eda12c4d8a8495da9ed124b0b0","8d10b7d3896e4cb389193a51a005ab08"]},"executionInfo":{"status":"ok","timestamp":1619374562551,"user_tz":-180,"elapsed":20098,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"409d1398-200e-4351-f73a-56f1ae6806d0"},"source":["train_data = torchvision.datasets.CIFAR10('./data',\n","    download=True,\n","    train=True,\n","    transform=None)\n","    \n","test_data = torchvision.datasets.CIFAR10('./data',\n","    download=True,\n","    train=False,\n","    transform=None)\n","\n","\n","\n","# X_train, X_validation, y_train, y_validation = train_test_split(train_data[: , 0], train_data[: , 1], test_size = validation_size)\n","# train_data = torchvision.datasets.MNIST(root ='', train=True ,download= True)\n","# test_data = torchvision.datasets.MNIST(root ='', train=False ,download= True)\n","# val_data = torchvision.datasets.MNIST(root ='', train=False ,download= True)\n","num_classes = len(train_data.classes)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d120651125294f9a8c9fd3f82a622b82","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mKZUkleJ63K","executionInfo":{"status":"ok","timestamp":1619374562552,"user_tz":-180,"elapsed":20074,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"d8e53457-2d4c-4827-de4b-06b0bd83a5eb"},"source":["type(train_data[0][0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PIL.Image.Image"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"jV9a_VDuJ8te"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XB2h_dCCBRJz"},"source":["# # len(train_data[0][1])\n","# plt.imshow(train_data[0][0].permute)\n","# display(train_data[0][1])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAn1q-yOCwqF"},"source":["# from keras.datasets import cifar10\n","\n","# (X, y), (_, _) = cifar10.load_data()\n","# # imagenet = [(image, lbl) for image, lbl, _ in zip(imagenet, imagenet_lbls, range(5))]\n","# X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQzj7dbR5_Hq"},"source":["### data class "]},{"cell_type":"markdown","metadata":{"id":"K3schKm76OMr"},"source":[""]},{"cell_type":"code","metadata":{"id":"7077MsXi2vC8"},"source":["# Must inherate from dataset\n","class cfarDataSet(Dataset):\n","    def __init__(self, x_train, y_train, transforms=None):\n","        super(cfarDataSet,self).__init__()\n","        # display(x_train)\n","        self.x_train = x_train\n","        self.y_train = y_train\n","        self.transforms = transforms\n","\n","        \n","    def __getitem__(self, idx):\n","        x = self.x_train[idx] / self.x_train[idx].max()\n","        y = self.y_train[idx]\n","\n","        if self.transforms:\n","            x = self.transforms(image=x.numpy())['image']\n","        \n","                # [Channel , H , W] -> Into The Network -> [Batch, Channel, Hight, Width]\n","\n","        \n","        return torch.Tensor(x).permute(2,1,0), y\n","\n","    def __len__(self):\n","        return len(self.x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeUeQYf3I8YH","executionInfo":{"status":"ok","timestamp":1619374564023,"user_tz":-180,"elapsed":21480,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"244147cb-2ab4-4ce0-ee00-95c27e58b083"},"source":["permute?"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Object `permute` not found.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zniiti35IvNE"},"source":["np.moveaxis?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUGTPmFk9ZDo"},"source":["# Taking only the first 600 values\n","\n","mnist_train = cfarDataSet(train_data.data[:600], train_data.targets[:600], transforms=None) \n","mnist_test = cfarDataSet(test_data.data[:600], test_data.targets[:600], transforms=None)\n","# mnist_val =  cfarDataSet(val_data.data[:600], val_data.targets[:600], transforms=None)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXa7SDrUKN13"},"source":["# mnist_test[5][0].permute(0,3,2,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DesWUNNB-vnc"},"source":["BS=10\n","mnist_loader_train = DataLoader(dataset = mnist_train, batch_size = BS, shuffle=True)\n","mnist_loader_test = DataLoader(dataset = mnist_test, batch_size = BS, shuffle=True)\n","# mnist_loader_val = DataLoader(dataset = mnist_val, batch_size = BS, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jPwIpkHYAR8C"},"source":[" class LeNetModel(nn.Module):\n","    def __init__(self):\n","        super(LeNetModel, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        # kernel\n","        self.conv0 = nn.Conv2d(3, 3, 5)\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        # self.pool = nn.MaxPool2d(2, 2)#######################################\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(256, 120)  # 6*6 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv0(x))\n","        x = F.relu(self.conv1(x))\n","        # x = self.pool(x) #######################################\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        # print(f'line17: x = {x}')\n","        x = x.view(x.shape[0], -1)\n","        # print(f'line19: x = {x}')\n","        x = F.relu(self.fc1(x))\n","        # print(f'line22 well well')\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nq-1DVYuCQXi"},"source":["lenet_model = LeNetModel()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lenet_model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)\n","opt2 = optim.SGD(lenet_model.parameters(), lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyYXTD8Len8H"},"source":["class AverageMeter(object):\n","    \"\"\"Compute and store the average & current value \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n \n","        self.count += n\n","        self.avg = self.sum / self.count\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qs60Gox3ML4R"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fw9zKmk2kNw-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619374578182,"user_tz":-180,"elapsed":2214,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"2593ad40-65e7-496f-97c9-33cbaf3c6dff"},"source":["from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard \n","# %reload_ext tensorboard\n","\n","\n","experiment = 1\n","run = 0 \n","writer = SummaryWriter(f'runs/CIFAR10_experiment_{experiment}')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w-aUB9N4MM8z"},"source":["%tensorboard --logdir=runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksOPjh1NX2m0"},"source":["\n","![image](https://user-images.githubusercontent.com/55464049/116005597-5fb02900-a610-11eb-9982-ecf725a9414c.png)\n","\n","![image](https://user-images.githubusercontent.com/55464049/116005622-7b1b3400-a610-11eb-88c1-8b476a68335b.png)\n","\n","Result: 84% accuracy"]},{"cell_type":"code","metadata":{"id":"mfK9B4LcARhf"},"source":[" def train_one_epoch_NOTUSED(dataloader , model, optimizer, epoc, criterion):\n","    for i, (image, target) in enumerate(dataloader):\n","        running_loss = 0\n","        running_acc = 0\n","        image = image.float() # float() will work better with pytorch (np.float will not run on the gpu)\n","        target = target.long()\n","        optimizer.zero_grad()\n","\n","        output = model(image) # forward pass \n","        loss = criterion(output, target) # Calculate loss\n","        \n","        loss.backward() # Calculate the derivetive \n","        optimizer.step() # will modify our network according to the new values.\n","        pred = output.argmax(dim = 1, keepdim = True) # getting the element with the highest probability\n","        acc = torchmetrics.functional.accuracy(pred, target) # reading the accuracy of the network\n","\n","        running_loss +=loss # for reporting the avg accuracy at the end\n","        running_acc += acc\n","        \n","    return running_loss/ len(dataloader.dataset), running_acc(dataloader.dataset)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuXLcuwKRPyG"},"source":["\n","\n","\n","def get_preds_and_probs(output):\n","    '''\n","    Generates predictions and corresponding probabilities from a trained\n","    network and a list of images\n","    '''\n","    # print('output shape', output.shape)\n","    # output = net(images)\n","    # convert output probabilities to predicted class\n","    _, preds_tensor = torch.max(output, 1)\n","    # print('preds_tensor',preds_tensor)\n","    argmax = np.squeeze(torch.argmax(output, dim=1, keepdim=True).detach().cpu().numpy())\n","    # print('argmax',argmax)\n","    probas = F.softmax(output,dim=1).detach().cpu().numpy()\n","    # print('probas shape before',probas.shape)\n","    #probas = np.array([probas[index] for i in range(probas.shape[0]))\n","    # print('probas shape',probas.shape)\n","    # print('probas datatype', type(probas[0]))\n","    # print(probas[0])\n","    \n","    proba = []\n","    for index, sample in enumerate(probas): \n","        proba.append(sample[argmax[index]])\n","\n","    #print('proba shape',len(proba))\n","    #print(proba)\n","    preds = np.squeeze(preds_tensor.detach().cpu().numpy())\n","    # print(preds, proba)\n","    return preds, proba\n","\n","\n","# def plot_classes_preds(image, model_outputs, labels):\n","\n","#     pred, probs = get_preds_and_probs(model_outputs)\n","\n","#     fig = plt.figure(figsize=(4*BS,4))\n","#     for idx in np.arange(BS):\n","#         ax = fig.add_subplot(1,BS, idx+1, xticks=[], yticks=[])\n","#         # plt.imshow(image[idx], one_channels=False)\n","#         # ax.set_title(\"{0}, 1:.1f)%\\n(label: {2})\".format(\n","#         #     classes[preds[idx]],\n","#         #     probs[idx] * 100.0,\n","#         #     classes[labels[idx]]),\n","#         #     color=(\"green\" if pres[idx] == labels[idx].item() else \"red\"))\n","        \n","#     return fig\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4BALX2LfSrh"},"source":["def train_one_epoch(dataloader , model, optimizer, epoch, criterion):\n","    accuracy = AverageMeter()\n","    losses = AverageMeter()\n","    model.train()\n","  \n","\n","\n","    for i, (image, target) in enumerate(dataloader):\n","        image = image.float() # float() will work better with pytorch (np.float will not run on the gpu)\n","        target = target.long()\n","        optimizer.zero_grad()\n","        output = model(image) # forward pass \n","        \n","\n","        loss = criterion(output, target) # Calculate loss\n","        loss.backward() # Calculate the derivetive \n","\n","        optimizer.step() # will modify our network according to the new values.\n","\n","\n","        pred = output.argmax(dim = 1, keepdim = True) # getting the element with the highest probability\n","        acc = torchmetrics.functional.accuracy(pred, target) # reading the accuracy of the network\n","        losses.update(loss.item(), image.size(0))\n","        accuracy.update(acc, image.size(0))\n","\n","        writer.add_figure('predictions vs. actuals',\n","                    plot_classes_preds(image, output, target),\n","                    global_step=epoch * len(dataloader) + i)\n","\n","        writer.add_scalar(' average training loss', losses.avg, epoch * len(dataloader) + i )\n","        writer.add_scalar(' training lose', losses.val, epoch * len(dataloader) + i )\n","        writer.add_scalar(' average training accuracy', accuracy.avg, epoch * len(dataloader) + i)\n","        writer.add_scalar(' training accuracy', accuracy.val, epoch * len(dataloader) + i)\n","        \n","    return losses.avg, accuracy.avg\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMn6VViVFVHh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619376140217,"user_tz":-180,"elapsed":296888,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"fd537b31-bced-4e82-f2cf-ce70c51eda29"},"source":["def train_n_epochs(num_f_epocs = 25):\n","    acc_dict = []\n","    loss_dict = []\n","    for i in range(num_f_epocs):\n","        losses, accuracy = train_one_epoch(mnist_loader_train, lenet_model, optimizer, i, criterion)\n","        loss_dict.append(losses)\n","        acc_dict.append(accuracy)\n","\n","    print(f'losses = {losses}')\n","    print(f'accuracy {accuracy}')\n","    return loss_dict, acc_dict\n","\n","loss_arr, acc_arr = train_n_epochs()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["losses = 0.5088760557274024\n","accuracy 0.8266666531562805\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WAvbbVxn1VL7"},"source":["lenet_model = LeNetModel()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lenet_model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)\n","opt2 = optim.SGD(lenet_model.parameters(), lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eee8AOfc1VMF"},"source":["def train_one_epoch(dataloader , model, optimizer, epoc, criterion):\n","    accuracy = AverageMeter()\n","    losses = AverageMeter()\n","    model.train()\n","\n","    global writer\n","\n","    for i, (image, target) in enumerate(dataloader):\n","        image = image.float() # float() will work better with pytorch (np.float will not run on the gpu)\n","        target = target.long()\n","        optimizer.zero_grad()\n","        output = model(image) # forward pass \n","\n","        loss = criterion(output, target) # Calculate loss\n","        loss.backward() # Calculate the derivetive \n","\n","        optimizer.step() # will modify our network according to the new values.\n","\n","        writer.add_figure('predictions vs. actuals',\n","                    plot_classes_preds(img, output, target),\n","                    global_step=epoch * len(trainloader) + i)\n","\n","\n","        pred = output.argmax(dim = 1, keepdim = True) # getting the element with the highest probability\n","        acc = torchmetrics.functional.accuracy(pred, target) # reading the accuracy of the network\n","        losses.update(loss.item(), image.size(0))\n","        accuracy.update(acc, image.size(0))\n","\n","        writer.add_scalar(' average training loss', losses.avg, epoch * len(trainlodaer) + i )\n","        writer.add_scalar(' training lose', losses.val, epoch * len(trainlodaer) + i )\n","        writer.add_scalar(' average training accuracy', accuracy.avg, epoch * len(trainlodaer) + i)\n","        writer.add_scalar(' training accuracy', accuracy.val, epoch * len(trainlodaer) + i)\n","\n","        \n","    return losses.avg, accuracy.avg\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOjqz7Ct1VMG"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"qI-JnQY3krgz","executionInfo":{"status":"ok","timestamp":1619376140223,"user_tz":-180,"elapsed":293397,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"6756d25f-635a-43bc-8660-0d24a4526400"},"source":["plt.plot(range(25), loss_arr, 'g', label='loss')\n","plt.plot(range(25), acc_arr, 'b', label='Accuracy')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"eV7uVfwdZ43U"},"source":["# Summary Report:"]},{"cell_type":"markdown","metadata":{"id":"gtbCWYwYZ7My"},"source":["# Copy from Submission 1 "]},{"cell_type":"markdown","metadata":{"id":"2r8NUiBLSxdW"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"ipEIFfGDECPx"},"source":["from matplotlib import pyplot as plt\n","import numpy as np\n","\n","import keras \n","from keras.datasets import mnist \n","(x_train, y_train), (x_test, y_test) = mnist.load_data() \n","\n","import cv2\n","from scipy.signal import convolve, convolve2d\n","import math\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGwpLxsWRyoh"},"source":["### Examples of augmentation class uses"]},{"cell_type":"code","metadata":{"id":"Ev4JpejRNb43"},"source":["def plot_images(images, titles, num_rows, width=15, height=15, padding=1.5, show_axis=False, color_map='gray'):\n","    fig, ax = plt.subplots(num_rows, len(images) // num_rows)\n","    if show_axis is False:\n","        [axi.set_axis_off() for axi in ax.ravel()]\n","    fig.set_figwidth(width)\n","    fig.set_figheight(height)\n","    fig.tight_layout(pad=padding)\n","\n","    for image, title, ax in zip(images, titles, ax.flatten()):\n","        ax.set_title(title)\n","        ax.imshow(image, cmap=color_map)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34ZCJH1oMVdc"},"source":["# Orgnize the data (Undo Shuffeling)\n","def organize_data(data, labels):\n","    # Initialize the list with 10 sublists (one for each digit)\n","    organized_data_list = [list() for _ in range(10)]\n","    # Iterate over every image and its label in the data\n","    for image, label in zip(data, labels):\n","        # Add the image to the appropappropriate sublist\n","        organized_data_list[label].append(image)\n","    return organized_data_list\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mwcpt0K1TNnB"},"source":["### Print examples of mnist dataset"]},{"cell_type":"code","metadata":{"id":"ibi6EBZhGB_0"},"source":["images_by_digit = organize_data(x_train, y_train)\n","plot_images([images_by_digit[i//5][i%5] for i in range(50)], (str(i//5) for i in range(50)), 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lG8cQwVHTdVW"},"source":["### concat n images togther"]},{"cell_type":"code","metadata":{"id":"HnagIeZYOp93"},"source":["# Source: https://stackoverflow.com/a/30228789\n","import numpy as np\n","\n","def concat_images(*images):\n","    if len(images) == 0:\n","        return None\n","        \n","    imgs_comb = np.hstack(images)\n","    return imgs_comb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3zr9dAeTkpj"},"source":["### Build dataset "]},{"cell_type":"code","metadata":{"id":"U6TtyQvBFMW1"},"source":["import random\n","# 000 : 4000 picutres\n","# 001:  4000 picures\n","\n","def build_data_set():\n","    def create_random_triplet_images(first_digit, second_digit, third_digit):\n","        first_digit_images = images_by_digit[first_digit]\n","        second_digit_images = images_by_digit[second_digit]\n","        third_digit_images = images_by_digit[third_digit]\n","        # return [concat_images(*image) for image, _ in zip(itertools.product(images_by_digit[0], images_by_digit[1], images_by_digit[0]), range(4000))]\n","        return ([concat_images(random.choice(first_digit_images),\n","                                random.choice(second_digit_images),\n","                                random.choice(third_digit_images))\n","                                for _ in range(4000)])\n","\n","    data_set = []\n","    for number in range(101):\n","        data_set.append(create_random_triplet_images(number // 100, (number // 10) % 10, number % 10))\n","    return data_set\n","\n","%time digit_triplets = build_data_set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o7XzFSFFTpYG"},"source":["### Present dataset samples"]},{"cell_type":"code","metadata":{"id":"UyZjmuf5aqMQ"},"source":["random_number = random.randint(0, 100)\n","plot_images([digit_triplets[random_number][i] for i in range(10)], (str(random_number) for _ in range(10)), 5, padding=0.3)\n","len(digit_triplets[random_number]) # 4000 :)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgYQpCrYsR1P"},"source":["### Building Dataset"]},{"cell_type":"code","metadata":{"id":"AyPkHrtUuhJR"},"source":["plt.imshow(digit_triplets[3][3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7YCA0NBu9q7"},"source":["def split_dataset_to_X_y_arrays(data):\n","    all_data_tupple = [] \n","    for number in range(len(data)): #101 numbers\n","        for img in (data[number]):\n","            all_data_tupple.append((img, number))\n","\n","    # convert to NP.array\n","    all_data_tupple = np.array(all_data_tupple)\n","    \n","    # shuffle \n","    random.shuffle(all_data_tupple) # Give it a g00d shuffle\n","\n","\n","    X = all_data_tupple[:, 0]\n","    y = all_data_tupple[:, 1]\n","    return X, y\n","\n","\n","def split_dataset_3way(X, y,validation_size = 0.30 ,test_size = 0.10):\n","    \"\"\"Will Split the data to Train, Test and Validation  \"\"\"\n","    # for example: 90% to training [= 0.1]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n","    # from that 90%, 10% will go to [ validation = 0.1 ] \n","    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = validation_size)\n","    # will leave us with:\n","    # 10% - Test\n","    # 81% - Train\n","    # 9% - Validation\n","    return X_train, X_validation, X_test, y_train, y_validation, y_test\n","\n","\n","\n","all_X, all_y = split_dataset_to_X_y_arrays(digit_triplets)\n","\n","X_train, X_valid, X_test, y_train, y_valid, y_test = split_dataset_3way(all_X, all_y, validation_size = 0.30 ,test_size = 0.10)\n","print(f'\\n\\nX_train:{len(X_train)}, X_valid:{len(X_valid)}, X_test:{len(X_test)},\\\n","    y_train:{len(y_train)}, y_valid:{len(y_valid)}, y_test:{len(y_test)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tiV30zt8zJ5s"},"source":["plt.imshow(X_train[5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zXL9qF8sTik"},"source":["# Must inherate from dataset\n","class mnist_3dig_DataSet(Dataset):\n","    def __init__(self, x_train, y_train, transforms=None):\n","        super(mnist_3dig_DataSet,self).__init__()\n","        self.x_train = x_train\n","        self.y_train = y_train\n","        self.transforms = transforms\n","\n","        \n","    def __getitem__(self, idx):\n","        x = self.x_train[idx] / self.x_train[idx].max()\n","        y = self.y_train[idx]\n","        if self.transforms:\n","            x = self.transforms(image=x.numpy())['image']\n","        \n","                # [Channel , H , W] -> Into The Network -> [Batch, Channel, Hight, Width]\n","        return x[np.newaxis, :, :], y\n","\n","    def __len__(self):\n","        return len(self.x_train)\n","\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fo0gTZ_Usfat"},"source":["# Taking only the first 600 values\n","\n","mnist_3dig_train = mnist_3dig_DataSet(X_train[:600], y_train[:600], transforms=None) \n","mnist_3dig_test = mnist_3dig_DataSet(X_test[:600],y_test[:600], transforms=None)\n","mnist_3dig_val =  mnist_3dig_DataSet(X_valid[:600], y_valid[:600], transforms=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qJiV87vsf5t"},"source":["### Building Pytorch's DataLodaer"]},{"cell_type":"code","metadata":{"id":"-Rt01Uposk8j"},"source":["BS=32\n","mnist_3dig_loader_train = DataLoader(dataset = mnist_3dig_train, batch_size = BS, shuffle=True)\n","mnist_3dig_loader_test = DataLoader(dataset = mnist_3dig_test, batch_size = BS, shuffle=True)\n","mnist_3dig_loader_val = DataLoader(dataset = mnist_3dig_val, batch_size = BS, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"evyHIOkIlrfJ"},"source":["## Building LeNet"]},{"cell_type":"code","metadata":{"id":"BNYSInpwaF17"},"source":[" class LeNetModel_3dig(nn.Module):\n","\n","    \"\"\"input size: [1(greyScale),84(width),28(height)]\"\"\"\n","    def __init__(self):\n","        super(LeNetModel_3dig, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(1152, 120)  # fixme\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 101)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2) # 84x28 -> 42x14\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2) # 42x14 -> 21x7\n","        # print(f'line17: x = {x}')\n","        x = x.view(x.shape[0], -1)\n","        # print(f'line19: x = {x}')\n","        x = F.relu(self.fc1(x))\n","        # print(f'line22 well well')\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ib7g20uwr6Qw"},"source":["lenet_model_3dig = LeNetModel_3dig()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lenet_model_3dig.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WH7z1Yjn1bJY"},"source":["def train_one_epoch(dataloader , model, optimizer, epoc, criterion):\n","    accuracy = AverageMeter()\n","    losses = AverageMeter()\n","    model.train()\n","  \n","\n","\n","    for i, (image, target) in enumerate(dataloader):\n","        image = image.float() # float() will work better with pytorch (np.float will not run on the gpu)\n","        target = target.long()\n","        optimizer.zero_grad()\n","        output = model(image) # forward pass \n","\n","        loss = criterion(output, target) # Calculate loss\n","        loss.backward() # Calculate the derivetive \n","\n","        optimizer.step() # will modify our network according to the new values.\n","\n","\n","        pred = output.argmax(dim = 1, keepdim = True) # getting the element with the highest probability\n","        acc = torchmetrics.functional.accuracy(pred, target) # reading the accuracy of the network\n","        losses.update(loss.item(), image.size(0))\n","        accuracy.update(acc, image.size(0))\n","       \n","        \n","    return losses.avg, accuracy.avg\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIBGb_cg1kIT"},"source":["from tqdm import tqdm\n","def train_n_epochs(num_f_epocs = 25):\n","    acc_dict = []\n","    loss_dict = []\n","    for i in tqdm(range(num_f_epocs)):\n","        losses, accuracy = train_one_epoch(mnist_3dig_loader_train, lenet_model_3dig, optimizer, i, criterion)\n","        loss_dict.append(losses)\n","        acc_dict.append(accuracy)\n","\n","    print(f'\\nlosses = {losses}')\n","    print(f'accuracy {accuracy}\\n\\n')\n","    return loss_dict, acc_dict\n","\n","loss_arr, acc_arr = train_n_epochs()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hymWC9lz12EX"},"source":["plt.plot(range(25), loss_arr, 'g', label='loss')\n","plt.plot(range(25), acc_arr, 'b', label='Accuracy')\n","plt.title('Loss and Accuracy loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gp8TfMQ4P5Oz"},"source":["### Check Validation accuracy"]},{"cell_type":"code","metadata":{"id":"d9Z8hqmMPA23"},"source":["def inference_date(dataloader , model, optimizer, epoc, criterion):\n","    accuracy = AverageMeter()\n","    losses = AverageMeter()\n","    model.train()\n","  \n","\n","\n","    for i, (image, target) in enumerate(dataloader):\n","        image = image.float() # float() will work better with pytorch (np.float will not run on the gpu)\n","        target = target.long()\n","        optimizer.zero_grad()\n","        output = model(image) # forward pass \n","\n","        loss = criterion(output, target) # Calculate loss\n","        # loss.backward() # Calculate the derivetive \n","\n","\n","\n","        pred = output.argmax(dim = 1, keepdim = True) # getting the element with the highest probability\n","        acc = torchmetrics.functional.accuracy(pred, target) # reading the accuracy of the network\n","        losses.update(loss.item(), image.size(0))\n","        accuracy.update(acc, image.size(0))\n","       \n","        \n","    return losses.avg, accuracy.avg\n","\n","losses, accuracy = inference_date(mnist_3dig_loader_val, lenet_model_3dig, optimizer, 1, criterion)\n","print(f'loss = {losses} , acc = {accuracy} ')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4e5Wic3QMXI"},"source":["def inference_single_image(single_image = mnist_3dig_val[1][0][0]):\n","    plt.imshow(single_image)\n","\n","    image = single_image.float() # float() will work better with pytorch (np.float will not run on the gpu)\n","    target = target.long()\n","    optimizer.zero_grad()\n","    output = model(image) # forward pass \n","    pred = output.argmax(dim = 1, keepdim = True) # getting the element with the highest probability\n","        \n","    return pred\n","\n","print(inference_single_image())      \n","       "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6lYIn6mJB1p"},"source":["### Save pytorch's state"]},{"cell_type":"code","metadata":{"id":"zxKVVBALJBBl"},"source":["def saveTorchState(epoch_num :int , model_state, optim_state, checkpoint_name = \"torchCheckPoint\" ):\n","    checkpoint = {\n","        \"epoch\":epoch_num, # An integer\n","        \"model_state\": model_state,  #model.state_dict(),\n","        \"optim_state\": optim_state #optimizer.state_dict()\n","    }\n","    \n","    torch.save(checkpoint, \"torchCheckPoint.pth\")\n","    \n","\n","def loadTorchState(file_name = \"torchCheckPoint.pth\"):\n","    loaded_checkpoint = torch.load(file_name)\n","    epoch = loaded_checkpoint[\"epoch\"]\n","\n","    # we must create our model again\n","    model = Model(n_input_features=6)\n","    optimizer = optim.Adam(lenet_model_3dig.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)\n","\n","    model.load_state_dict(checkpoint[\"model_state\"])\n","    optimizer.load_state_dict(checkpoint[\"optim_state\"])\n","\n","    return epoch, model, optimizer\n","\n","\n","# ######## When using GPU (cuda) ########\n","\n","# # ___ Save on GPU & Load on CPU ___\n","# #save\n","# device = torch.device(\"cuda\")\n","# model.to(device)\n","# torch.save(model.state_dict(),PATH)\n","\n","# #load\n","# device = torch.device(\"cpu\")\n","# model = Model(*args, **kwargs)\n","# model.load_state_dict(torch.load(PATH, map_location=device))\n","\n","# # ___ Both save & load on GPU ___\n","# #save\n","# device = torch.device(\"cuda\")\n","# model.to(device)\n","# torch.save(model.state_dict(),PATH)\n","\n","# #load\n","# model = Model(*args, **kwargs)\n","# model.load_state_dict(torch.load(PATH))\n","# device = torch.device(\"cpu\")\n","\n","# # ___ Save on CPU & Load on GPU ___\n","# #save \n","# torch.save(model.state_dict(), PATH)\n","\n","# #load\n","# device = torch.device(\"cuda\")\n","# model = Model(*args, **kwargs)\n","# model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n","# model.to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_hQkVJjMKcid"},"source":["### Save& Load pytorch's State"]},{"cell_type":"code","metadata":{"id":"ynhZwrSgH_4Y"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yio3eorx7_Tf"},"source":["# Lab8 "]},{"cell_type":"code","metadata":{"id":"iW70WwHe8mNq"},"source":["trainset = torchvision.datasets.CIFAR10('./data',\n","    download=True,\n","    train=True,\n","    transform=transform)\n","testset = torchvision.datasets.CIFAR10('./data',\n","    download=True,\n","    train=False,\n","    transform=transform)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkgH6nPq2It0"},"source":["## tensorBoard"]},{"cell_type":"markdown","metadata":{"id":"4R9gfkEp2ocn"},"source":["###Init tensor"]},{"cell_type":"code","metadata":{"id":"en6kB9yk2KzX"},"source":["from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard # Load tensor into colab\n","%reload_ext tensorboard # incase somthing went wrong (MAKING SURE THE PREVIUS LINE DIDNT WORK)\n","\n","experiment = 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3g4ny-e2WYl"},"source":["experiment += 1 \n","experiment += 1 \n","run = 0\n","writer = SummaryWriter(f'runs/CIFAR10_experiment_{experiment}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50mbOJIO97lu"},"source":["%tensorboard --logdir=name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_PHsbh_Ks4s"},"source":["def get_preds_and_probs(output):\n","    '''\n","    Generates predictions and corresponding probabilities from a trained\n","    network and a list of images\n","    '''\n","    # print('output shape', output.shape)\n","    # output = net(images)\n","    # convert output probabilities to predicted class\n","    _, preds_tensor = torch.max(output, 1)\n","    # print('preds_tensor',preds_tensor)\n","    argmax = np.squeeze(torch.argmax(output, dim=1, keepdim=True).detach().cpu().numpy())\n","    # print('argmax',argmax)\n","    probas = F.softmax(output,dim=1).detach().cpu().numpy()\n","    # print('probas shape before',probas.shape)\n","    #probas = np.array([probas[index] for i in range(probas.shape[0]))\n","    # print('probas shape',probas.shape)\n","    # print('probas datatype', type(probas[0]))\n","    # print(probas[0])\n","    \n","    proba = []\n","    for index, sample in enumerate(probas): \n","        proba.append(sample[argmax[index]])\n","\n","    #print('proba shape',len(proba))\n","    #print(proba)\n","    preds = np.squeeze(preds_tensor.detach().cpu().numpy())\n","    # print(preds, proba)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWxsxAGn9A2X"},"source":["writer.add_scalar(' avrage training loss', losses.avg, epoch * len(trainloader) + i)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qE9rbFhV9SGU"},"source":["#### [Reference](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"]},{"cell_type":"code","metadata":{"id":"c0K9RTl49Avh"},"source":["mport torch\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset , DataLoader\n","from sklearn.metrics import confusion_matrix\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import plot_confusion_matrix\n","from torch.autograd import Variable\n","\n","%matplotlib inline\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YZK5_i69s-B"},"source":["# not that alexnet does not excpect this \n","model = torchvision.models.alexnet(pretrained=True) # will download the model \n","\n","# we must work with lenet\n","# (lenet should be best)\n","\n","# or work with other architectures "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7W-ZzKFEWdTr","executionInfo":{"status":"ok","timestamp":1619376888511,"user_tz":-180,"elapsed":1879,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"98c32870-8dca-4559-d134-0dd17acac3f1"},"source":["%%shell\n","\n","jupyter nbconvert --to html /content/Lab8.ipynb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[NbConvertApp] Converting notebook /content/Lab8.ipynb to html\n","[NbConvertApp] Writing 498120 bytes to /content/Lab8.html\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"id":"2-Z6IEi3W_Gk","executionInfo":{"status":"error","timestamp":1621317991195,"user_tz":-180,"elapsed":2685,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"dccf9f78-c835-431b-c612-06f433dc59c1","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%shell\n","\n","jupyter nbconvert --to html /content/Lab8.ipynb"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[NbConvertApp] WARNING | pattern u'/content/Lab8.ipynb' matched no files\n","This application is used to convert notebook files (*.ipynb) to various other\n","formats.\n","\n","WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n","\n","Options\n","-------\n","\n","Arguments that take values are actually convenience aliases to full\n","Configurables, whose aliases are listed on the help line. For more information\n","on full configurables, see '--help-all'.\n","\n","--execute\n","    Execute the notebook prior to export.\n","--allow-errors\n","    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n","--no-input\n","    Exclude input cells and output prompts from converted document. \n","    This mode is ideal for generating code-free reports.\n","--stdout\n","    Write notebook output to stdout instead of files.\n","--stdin\n","    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n","--inplace\n","    Run nbconvert in place, overwriting the existing notebook (only \n","    relevant when converting to notebook format)\n","-y\n","    Answer yes to any questions instead of prompting.\n","--clear-output\n","    Clear output of current file and save in place, \n","    overwriting the existing notebook.\n","--debug\n","    set log level to logging.DEBUG (maximize logging output)\n","--no-prompt\n","    Exclude input and output prompts from converted document.\n","--generate-config\n","    generate default config file\n","--nbformat=<Enum> (NotebookExporter.nbformat_version)\n","    Default: 4\n","    Choices: [1, 2, 3, 4]\n","    The nbformat version to write. Use this to downgrade notebooks.\n","--output-dir=<Unicode> (FilesWriter.build_directory)\n","    Default: ''\n","    Directory to write output(s) to. Defaults to output to the directory of each\n","    notebook. To recover previous default behaviour (outputting to the current\n","    working directory) use . as the flag value.\n","--writer=<DottedObjectName> (NbConvertApp.writer_class)\n","    Default: 'FilesWriter'\n","    Writer class used to write the  results of the conversion\n","--log-level=<Enum> (Application.log_level)\n","    Default: 30\n","    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n","    Set the log level by value or name.\n","--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n","    Default: u''\n","    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n","    but can be any url pointing to a copy  of reveal.js.\n","    For speaker notes to work, this must be a relative path to a local  copy of\n","    reveal.js: e.g., \"reveal.js\".\n","    If a relative path is given, it must be a subdirectory of the current\n","    directory (from which the server is run).\n","    See the usage documentation\n","    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n","    slideshow) for more details.\n","--to=<Unicode> (NbConvertApp.export_format)\n","    Default: 'html'\n","    The export format to be used, either one of the built-in formats\n","    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n","    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n","    the import path for an `Exporter` class\n","--template=<Unicode> (TemplateExporter.template_file)\n","    Default: u''\n","    Name of the template file to use\n","--output=<Unicode> (NbConvertApp.output_base)\n","    Default: ''\n","    overwrite base name use for output files. can only be used when converting\n","    one notebook at a time.\n","--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n","    Default: u''\n","    PostProcessor class used to write the results of the conversion\n","--config=<Unicode> (JupyterApp.config_file)\n","    Default: u''\n","    Full path of a config file.\n","\n","To see all available configurables, use `--help-all`\n","\n","Examples\n","--------\n","\n","    The simplest way to use nbconvert is\n","    \n","    > jupyter nbconvert mynotebook.ipynb\n","    \n","    which will convert mynotebook.ipynb to the default format (probably HTML).\n","    \n","    You can specify the export format with `--to`.\n","    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n","    \n","    > jupyter nbconvert --to latex mynotebook.ipynb\n","    \n","    Both HTML and LaTeX support multiple output templates. LaTeX includes\n","    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n","    can specify the flavor of the format used.\n","    \n","    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n","    \n","    You can also pipe the output to stdout, rather than a file\n","    \n","    > jupyter nbconvert mynotebook.ipynb --stdout\n","    \n","    PDF is generated via latex\n","    \n","    > jupyter nbconvert mynotebook.ipynb --to pdf\n","    \n","    You can get (and serve) a Reveal.js-powered slideshow\n","    \n","    > jupyter nbconvert myslides.ipynb --to slides --post serve\n","    \n","    Multiple notebooks can be given at the command line in a couple of \n","    different ways:\n","    \n","    > jupyter nbconvert notebook*.ipynb\n","    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n","    \n","    or you can specify the notebooks list in a config file, containing::\n","    \n","        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n","    \n","    > jupyter nbconvert --config mycfg.py\n","\n"],"name":"stdout"},{"output_type":"error","ename":"CalledProcessError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-23210fa4fab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\njupyter nbconvert --to html /content/Lab8.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command '\njupyter nbconvert --to html /content/Lab8.ipynb' returned non-zero exit status 255."]}]},{"cell_type":"code","metadata":{"id":"n_0bglxYFQsp"},"source":[""],"execution_count":null,"outputs":[]}]}