{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab7 Prep Shahar Raz.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMoT5a54bmEvAW8Epd+NMKZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WUHj53A5jM5O"},"source":["# Assigment Paper:\n","\n","![image](https://user-images.githubusercontent.com/55464049/114030959-8c7be680-9883-11eb-932b-44146b041417.png)\n","\n","\n","[Adam](https://arxiv.org/pdf/1412.6980v1.pdf)\n","\n","[AdamW](https://arxiv.org/abs/1711.05101v3)\n","\n","\n","[Focal Loss](https://arxiv.org/abs/1708.02002v2)\n","\n","[Dice](https://arxiv.org/pdf/1707.03237.pdf)\n","\n","Binary Cross Entropy with logits loss "]},{"cell_type":"markdown","metadata":{"id":"2rGgUaNPFW1k"},"source":["# Lec Preperation"]},{"cell_type":"markdown","metadata":{"id":"VWqc9K2QkcFD"},"source":["##1. Optimizers Read & Summarize "]},{"cell_type":"markdown","metadata":{"id":"yFuQzxgeFbQS"},"source":["### 1.1 Adam\n","a method for efficient stochastic optimization that only requires first-order gradients and requires little memory.\n","\n","Consists of 2 main methods for reaching fast to the minimum.\n","\n","1. Momentum: keeping the general direction of the movement. helps avoiding local minimums, and acts like a ball goin down a hill.\n","![image](https://user-images.githubusercontent.com/55464049/114276061-b244ef00-9a2d-11eb-9c33-feaa59b257a7.png)\n","\n","\n","\n","2. RMSProp: (a better version of AdaGrad which keeping track of a historical average of square of gradient values)\n","\n","![image](https://user-images.githubusercontent.com/55464049/114275988-5e3a0a80-9a2d-11eb-854c-d0af3e60f02a.png)\n",", in RMSProp we also add friction (decay) \n","![image](https://user-images.githubusercontent.com/55464049/114275942-14e9bb00-9a2d-11eb-97b8-c200f235ba48.png)\n","which will not allow the values to explode.\n","The RMSProp arrests progress along the fast moving direction while simultaneously accelerating progress along slow moving directions, which helps it to bend directly towards the bottom.\n","\n","\n","\n","\n","\n","![image](https://user-images.githubusercontent.com/55464049/114276907-6d22bc00-9a31-11eb-854a-d903d36e5793.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Cfqvs_4kjOKA"},"source":["### 1.2 AdamW Optimizer\n","A simple modification from Adam to recover the original formulation of weight decay regularization by __decoupling__ the weight decay from the optimization steps taken. The weight decay is calculated _after_ the main calculation, and therefore does not interrupt with the them.\n","\n","[Link](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)\n","![image](https://user-images.githubusercontent.com/55464049/114277470-fe932d80-9a33-11eb-9158-9967d370654e.png)\n"]},{"cell_type":"markdown","metadata":{"id":"TzEDZJK2e6cb"},"source":["### 1.3 SGD\n","Basically, when we want to progress with our model:\n","* we can check one example, check the error, compute the gradients and change the network. but, one example can be biased in many ways, and contains an unknown amount of noise. __1 example__.\n","\n","* So in order to find patterns, and go in the right direction, most likely is that we'll want to check all of our examples, average the direction, compute the loss and backdrop according to all of it.\n","but, if we have a lot of data (millions-billions..) computing and averaging all the examples for each step will take VERY much time. __N examples__.\n","\n","* so, we try to find a middle way, here is where __SGD__ comes in! \n","We'll randomly subsample our dataset to groups who each have k samples in it.\n","So rather than computing the loss on the full data-set, in each step we'll compute the loss over a minibatch, and backprop accordingly. __n examples__.\n","\n","![image](https://user-images.githubusercontent.com/55464049/114276150-2384a200-9a2e-11eb-9c8d-fc12851f536c.png)\n"]},{"cell_type":"markdown","metadata":{"id":"-sE1buYMk8HP"},"source":["## 2. Optimizer Implemetations"]},{"cell_type":"markdown","metadata":{"id":"JLN62QIootAK"},"source":["### 2.1 AdamW Implementation"]},{"cell_type":"code","metadata":{"id":"VcfSUnjmjLG_"},"source":["# from: https://github.com/egg-west/AdamW-pytorch/blob/master/adamW.py\n","import math\n","import torch\n","from torch.optim.optimizer import Optimizer\n","\n","class AdamW(Optimizer):\n","    \"\"\"Implements Adam algorithm.\n","    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n","            algorithm from the paper `On the Convergence of Adam and Beyond`_\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _On the Convergence of Adam and Beyond:\n","        https://openreview.net/forum?id=ryQu7f-RZ\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0, amsgrad=False):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad)\n","        super(AdamW, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamW, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","                amsgrad = group['amsgrad']\n","\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","                    if amsgrad:\n","                        # Maintains max of all exp. moving avg. of sq. grad. values\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                # if group['weight_decay'] != 0:\n","                #     grad = grad.add(group['weight_decay'], p.data)\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    # Maintains the maximum of all 2nd moment running avg. till now\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    # Use the max. for normalizing running avg. of gradient\n","                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n","                else:\n","                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                # p.data.addcdiv_(-step_size, exp_avg, denom)\n","                p.data.add_(-step_size,  torch.mul(p.data, group['weight_decay']).addcdiv_(1, exp_avg, denom) )\n","\n","        return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYjq9UqvquoP"},"source":["### 2.2 Adam"]},{"cell_type":"code","metadata":{"id":"fUGDz16Yof04"},"source":["import torch\n","import math\n","from torch.optim import Adam, SGD"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBBWjGFhrkzQ"},"source":["### 2.3 Choose Optimizer:"]},{"cell_type":"code","metadata":{"id":"Rw354jIVrnOH"},"source":["LR = 0.01\n","BATCH_SIZE = 32\n","EPOCH = 12\n","WD = 0.1\n","\n","def set_optimizer(opt_name, params, learningRate):\n","    if opt_name == 'SGD':\n","        optimizer = optim.SGD(params, lr=learningRate)\n","    elif opt_name == 'Adam':\n","        optimizer = optim.Adam(params, lr=learningRate)\n","    elif opt_name == 'AdamW':\n","        optimizer = AdamW(params, lr=learningRate, betas=(0.9, 0.99), weight_decay = 0.1)\n","    else:\n","        print(f'no {opt_name} optimizer availble, choosing Adam by default.')\n","        optimizer = optim.Adam(params, lr=learningRate)\n","    \n","    if optimizer:\n","        return optimize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlyGZy2cq1LS"},"source":["## 3. Losses Read & Summarize "]},{"cell_type":"markdown","metadata":{"id":"0_EcZzjKrYzh"},"source":["### 3.1 Cross Entropy - CE\n","The Cross Entropy function is a way to measure the loss of classification problem. It must be activated after softmax function, which calculates the odds for specific class to be choosen. If the network is 100% sure of a class, it will give it score of 1, and in this case we will have no loss, which means that the NN will not be modified.\n","The Step size is accoding to the loss, if the loss is high, the CE derivetive will be steep, therefor the step size will be bigger.\n","\n","[Stat-Quest Video](https://www.youtube.com/watch?v=6ArSys5qHAU&ab_channel=StatQuestwithJoshStarmer)\n","\n","![image](https://user-images.githubusercontent.com/55464049/114303193-9563f680-9ad5-11eb-9a0b-f6cd93b6ddad.png)\n"]},{"cell_type":"markdown","metadata":{"id":"Tgp0QyMyrdfo"},"source":["### 3.2 Binary Cross Entropy with logits loss\n","\n","[What is Logits](https://stackoverflow.com/questions/34240703/what-is-logits-what-is-the-difference-between-softmax-and-softmax-cross-entropy)\n","\n","\n","* Logits:  the function operates on the unscaled output, that __the values are not probabilities__.\n","\n","* Binary Cross Entropy: Cross Entropy of only 2 categories.\n","\n","\n","[Pytorch Link:](https://pytorch.org/docs/master/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)\n","This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qrwzi6FctvAw"},"source":["### 3.3 Focal Loss\n","Focal Loss is a modulated CE function which reduces the loss for easy / common examples, whereas keep the loss for the hard / rare examples.\n","When __Gamma==0 <=> FocalLoss == CE__ , when gamma \n","\n","this approach is useful in many Image Classification Problems.\n","![image](https://user-images.githubusercontent.com/55464049/114303741-4f5c6200-9ad8-11eb-8edd-5ec9e6cfd095.png)\n","\n","Another way, apart from Focal Loss, to deal with class imbalance is to introduce weights. Give high weights to the rare class and small weights to the dominating or common class. These weights are referred to as α.\n","\n","![image](https://user-images.githubusercontent.com/55464049/114303812-aa8e5480-9ad8-11eb-9bf2-b56d8f29620b.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XJcJr2cAtxSS"},"source":["### 3.4 Dice\n","\n","Dice loss is based on the Sorensen-Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.\n","\n","Dice's formula:\n","![image](https://user-images.githubusercontent.com/55464049/114304068-e83fad00-9ad9-11eb-9bb9-3ab066dddd02.png)\n","\n","[Dice impelemtation:](https://www.jeremyjordan.me/semantic-segmentation/#:~:text=around%20the%20cells.%20(-,Source),denotes%20perfect%20and%20complete%20overlap.)"]},{"cell_type":"code","metadata":{"id":"chJqu5vv5TvD"},"source":["# from : https://www.jeremyjordan.me/semantic-segmentation/#:~:text=around%20the%20cells.%20(-,Source),denotes%20perfect%20and%20complete%20overlap.\n","def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n","    # skip the batch and class axis for calculating Dice score\n","    axes = tuple(range(1, len(y_pred.shape)-1)) \n","    numerator = 2. * np.sum(y_pred * y_true, axes)\n","    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n","    \n","    return 1 - np.mean((numerator + epsilon) / (denominator + epsilon)) # average over classes and batch\n","    # thanks @mfernezir for catching a bug in an earlier version of this implementation!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HtFZd5trGzd"},"source":["## 4. Losses Implementations:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R2LojdxJrUXg"},"source":["### 4.1  Focal Loss\n"]},{"cell_type":"code","metadata":{"id":"sVY-owMBrT9J"},"source":["def focal_loss(pt, alpha, gama):\n","    return (-1) * alpha * ((1 - pt)**gama) * np.log(pt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKlJWRvAfGwy"},"source":["## Snippets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"mbyFeglcfJU6","executionInfo":{"status":"ok","timestamp":1618140161597,"user_tz":-180,"elapsed":2203,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"81206945-02fb-45bd-896d-371a311c0f38"},"source":["import torch\n","import math\n","from torch.optim import Adam, SGD\n","from torch.optim.optimizer import Optimizer, required\n","\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(D_in, H),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(H, D_out),\n",")\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","\n","learning_rate = 1e-4\n","\n","display(model.parameters())\n","for parm in model.parameters():\n","    print(f' {parm.shape} \\t')\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","for t in range(500):\n","\n","    y_pred = model(x)\n","\n","    loss = loss_fn(y_pred, y)\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<generator object Module.parameters at 0x7f61f790e2d0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":[" torch.Size([100, 1000]) \t\n"," torch.Size([100]) \t\n"," torch.Size([10, 100]) \t\n"," torch.Size([10]) \t\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"],"name":"stderr"},{"output_type":"stream","text":["99 62.18852996826172\n","199 1.151323676109314\n","299 0.011353669688105583\n","399 0.000170590152265504\n","499 1.551429932078463e-06\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s8S3J1Tefdfr","executionInfo":{"status":"ok","timestamp":1618144184301,"user_tz":-180,"elapsed":2094,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"ee702861-8fcd-41ee-e041-c2d4e5fc3fd5"},"source":["%%shell\n","jupyter nbconvert --to html /content/Lab7_Prep_Shahar_Raz.ipynb"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[NbConvertApp] Converting notebook /content/Lab7_Prep_Shahar_Raz.ipynb to html\n","[NbConvertApp] Writing 317815 bytes to /content/Lab7_Prep_Shahar_Raz.html\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"OYIpNCfR0vLd"},"source":["# IN Class Practice:"]},{"cell_type":"code","metadata":{"id":"7AaIbUUWfLAR"},"source":["import torch\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import tensorflow.keras.datasets.mnist as mnist\n","from torch.utils.data import Dataset , DataLoader\n","from sklearn.metrics import confusion_matrix\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import plot_confusion_matrix\n","from torch.autograd import Variable\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUVSzdu202ra","executionInfo":{"status":"ok","timestamp":1618159583353,"user_tz":-180,"elapsed":10386,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"d84a8aba-cfaa-4cd6-8644-c94d7b298cf2"},"source":["!pip install torchmetrics\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchmetrics\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n","\r\u001b[K     |█▉                              | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 22.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 102kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 133kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 163kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 174kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.2.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bpYBknUc2eA8"},"source":["### How to step the optimizer"]},{"cell_type":"code","metadata":{"id":"agMfTrRz02jW"},"source":["import torchmetrics\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvHfkiAP05hh"},"source":["model = nn.Linear(1,1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjWPM9Mo1Ef3"},"source":["sgd = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9, nesterov=True)\n","# momentum : Parameter that accelerate SGD in relevant direction and dampens oscillations\n","sgd.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PFQb76C21blI"},"source":["adam = optim.Adam(model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)\n","adam.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"lIVP_aNt19aI","executionInfo":{"status":"error","timestamp":1618159933096,"user_tz":-180,"elapsed":1301,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"c4a219d0-e428-4798-d39a-71db3549af64"},"source":["adamw = optim.AdamW(model.parameters(),lr = 1e-3)\n","adamw.step(closure=None)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d47ff61e3d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0madamw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madamw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mamsgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                     \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                     \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'beta1' referenced before assignment"]}]},{"cell_type":"markdown","metadata":{"id":"6Zvvjg3f2hGL"},"source":["### import data"]},{"cell_type":"code","metadata":{"id":"uvpRTFAk2Of4"},"source":["train_data = torchvision.datasets.MNIST(root ='', train=True ,download= True)\n","test_data = torchvision.datasets.MNIST(root ='', train=False ,download= True)\n","val_data = torchvision.datasets.MNIST(root ='', train=False ,download= True)\n","num_classes = len(train_data.classes)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQzj7dbR5_Hq"},"source":["### data class "]},{"cell_type":"markdown","metadata":{"id":"K3schKm76OMr"},"source":[""]},{"cell_type":"code","metadata":{"id":"7077MsXi2vC8"},"source":["# Must inherate from dataset\n","class mnistDataSet(Dataset):\n","    def __init__(self, x_train, y_train, transforms=None):\n","        super(mnistDataSet,self).__init__()\n","        self.x_train = x_train\n","        self.y_train = y_train\n","        \n","    def __getitem__(self, idx):\n","        x = self.x_train[idx] / np.max(x_train[idx])\n","        y = self.y_train[idx]\n","        if self.transforms:\n","            x = self.transforms(image=x.numpy())['image']\n","        \n","                # [Channel , H , W] -> Into The Network -> [Batch, Channel, Hight, Width]\n","        return x[np.newaxis, :, :], y\n","\n","    def __len__(self):\n","        return len(self.x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUGTPmFk9ZDo"},"source":["# Taking only the first 600 values\n","\n","mnist_train = mnistDataSet(train_data.data[:600], train_data.targets[:600], transforms=None) \n","mnist_test = mnistDataSet(test_data.data[:600], test_data.targets[:600], transforms=None)\n","mnist_val =  mnistDataSet(val_data.data[:600], val_data.targets[:600], transforms=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DesWUNNB-vnc"},"source":["BS=32\n","mnist_loader_train = DataLoader(dataset = mnist_train, batch_size = BS, shuffle=True)\n","mnist_loader_test = DataLoader(dataset = mnist_test, batch_size = BS, shuffle=True)\n","mnist_loader_val = DataLoader(dataset = mnist_val, batch_size = BS, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"jPwIpkHYAR8C","executionInfo":{"status":"error","timestamp":1618164151882,"user_tz":-180,"elapsed":1020,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"1f9cd06e-82e0-44f1-ad89-e697f819f712"},"source":[" class LeNetModel():\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 3)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        print(f'line17: x = {x}')\n","        x = x.view(x.shape[0], -1)\n","        print(f'line19: x = {x}')\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-4262137119ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLeNetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m        \u001b[0;31m# 1 input image channel, 6 output channels, 3x3 square convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;31m# kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"id":"nq-1DVYuCQXi","executionInfo":{"status":"error","timestamp":1618164153254,"user_tz":-180,"elapsed":1989,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"91a6ef91-916c-421f-f823-5f639954eb7c"},"source":["lenet = LeNetModel()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lenet.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay=0, amsgrad=False)\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-90e107e5deed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-5031ec4bd1e2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLeNetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m        \u001b[0;31m# 1 input image channel, 6 output channels, 3x3 square convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;31m# kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Net' is not defined"]}]},{"cell_type":"code","metadata":{"id":"mfK9B4LcARhf"},"source":["def train_one_epoch(train_loader, model, optimizer, epoc, riterion):\n","    for i, (image, target) in enumerate(train_lodaer):\n","        running_loss = 0\n","        running_acc = 0\n","        img = img.float() # float() will work better with pytorch (np.float will not run on the gpu)\n","        target = target.long()\n","        optimizer.zero_grad()\n","\n","        output = model(img) # forward pass \n","        loss = criterion(output, target) # Calculate loss\n","        \n","        loss.backward() # Calculate the derivetive \n","        optimizer.step() # will modify our network according to the new values.\n","        pred = output.argmax(dim = 1, keepdim = True) # getting the element with the highest probability\n","        acc = torchmetrics.functional.accuracy(pred, target) # reading the accuracy of the network\n","\n","        running_loss +=loss # for reporting the avg accuracy at the end\n","        running_acc += acc\n","        \n","    return running_loss/ len(dataloader.dataset), running_acc(dataloader.dataset)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"PMn6VViVFVHh","executionInfo":{"status":"error","timestamp":1618164154285,"user_tz":-180,"elapsed":1177,"user":{"displayName":"Shahar Raz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbVeU9ikiZjf4mwWC_01xPXpX-Hd4DE0MfVc_D=s64","userId":"08841176503165409785"}},"outputId":"9292e096-f623-4f31-f9d4-87ba0374143b"},"source":["for i in range(25):\n","    train_one_epoch()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-73f08c4eaaac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: train_one_epoch() missing 5 required positional arguments: 'train_loader', 'model', 'optimizer', 'epoc', and 'riterion'"]}]},{"cell_type":"code","metadata":{"id":"IVI6mzeKGXnZ"},"source":[""],"execution_count":null,"outputs":[]}]}